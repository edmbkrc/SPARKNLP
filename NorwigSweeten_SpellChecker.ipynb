{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7plRiT7YcrU",
        "outputId": "2d1796ac-34bc-4525-eb48-eaad9a5e231e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.5/579.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiwQJWHXZDCT"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "from sparknlp import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = sparknlp.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "spellchecker = NorvigSweetingModel.pretrained(\"spellcheck_norvig\") \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell\")\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "documentAssembler,\n",
        "tokenizer,\n",
        "spellchecker\n",
        "])\n",
        "\n",
        "data = spark.createDataFrame([[\"somtimes i wrrite wordz erong.\"]]).toDF(\"text\")\n",
        "result = pipeline.fit(data).transform(data)\n",
        "result.select(col(\"token.result\").alias(\"before_spellchecker\"), col(\"spell.result\").alias(\"after_spellchecker\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3llP2gelN7y",
        "outputId": "8f9ea919-1130-4c8a-c47f-c6bad89be9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spellcheck_norvig download started this may take some time.\n",
            "Approximate size to download 4.2 MB\n",
            "[OK!]\n",
            "+--------------------------------------+--------------------------------------+\n",
            "|before_spellchecker                   |after_spellchecker                    |\n",
            "+--------------------------------------+--------------------------------------+\n",
            "|[somtimes, i, wrrite, wordz, erong, .]|[sometimes, i, write, words, wrong, .]|\n",
            "+--------------------------------------+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a spellchecker with the NorvigSweetingModel"
      ],
      "metadata": {
        "id": "-kAXSCqepZvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_dict = \"\"\"\n",
        "dog\n",
        "fish\n",
        "horse\n",
        "\"\"\"\n",
        "\n",
        "with open(\"external_dict.txt\", \"w\") as f:\n",
        "  f.write(external_dict)\n",
        "\n",
        "! head external_dict.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDZ3MnyapdqM",
        "outputId": "6d17fb21-4623-491c-8df7-29bf33b0c855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dog\n",
            "fish\n",
            "horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "spellchecker = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell\") \\\n",
        ".setDictionary(\"external_dict.txt\")\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "documentAssembler,\n",
        "tokenizer,\n",
        "spellchecker\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "spellcheck_model = pipeline.fit(empty_df)\n",
        "\n",
        "text_df = spark.createDataFrame([[\"The dogh is eating.\"]]).toDF(\"text\")\n",
        "\n",
        "corrected_text = spellcheck_model.transform(text_df)\n",
        "\n",
        "corrected_text.select(col(\"token.result\").alias(\"before_spellchecker\"), col(\"spell.result\").alias(\"after_spellchecker\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-59ssPnipzax",
        "outputId": "461b071f-46c1-4eb6-a58b-9d86c1d931d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-------------------------+\n",
            "|before_spellchecker       |after_spellchecker       |\n",
            "+--------------------------+-------------------------+\n",
            "|[The, dogh, is, eating, .]|[The, dog, is, eating, .]|\n",
            "+--------------------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setCaseSensitive"
      ],
      "metadata": {
        "id": "0_NPLIfgvabJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capital_external_dict = \"\"\"\n",
        "Dog\n",
        "Fish\n",
        "Horse\n",
        "\"\"\"\n",
        "\n",
        "with open(\"capital_external_dict.txt\", \"w\") as f:\n",
        "  f.write(capital_external_dict)\n",
        "\n",
        "! head capital_external_dict.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A84ThZcOtKVF",
        "outputId": "539acf03-7251-459e-ad6e-417e75363c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dog\n",
            "Fish\n",
            "Horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "spellchecker_1 = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell_1\") \\\n",
        ".setDictionary(\"capital_external_dict.txt\") \\\n",
        ".setCaseSensitive(False)\n",
        "\n",
        "spellchecker_2 = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell_2\") \\\n",
        ".setDictionary(\"capital_external_dict.txt\") \\\n",
        ".setCaseSensitive(True)\n",
        "\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "documentAssembler,\n",
        "tokenizer,\n",
        "spellchecker_1,\n",
        "spellchecker_2\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "spellcheck_model = pipeline.fit(empty_df)\n",
        "\n",
        "text_df = spark.createDataFrame([[\"The name of the dogh is Dogh.\"]]).toDF(\"text\")\n",
        "\n",
        "corrected_text = spellcheck_model.transform(text_df)\n",
        "\n",
        "corrected_text.select(col(\"token.result\").alias(\"before_spellchecker\"), col(\"spell_1.result\").alias(\"case_sensitive_false\"), col(\"spell_2.result\").alias(\"case_sensitive_true\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygjyh_AivZn_",
        "outputId": "588ea5de-bcd6-405c-ee80-15fe674df486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------+--------------------------------------+--------------------------------------+\n",
            "|before_spellchecker                    |case_sensitive_false                  |case_sensitive_true                   |\n",
            "+---------------------------------------+--------------------------------------+--------------------------------------+\n",
            "|[The, name, of, the, dogh, is, Dogh, .]|[The, name, of, the, dog, is, Dogh, .]|[The, name, of, the, dogh, is, Dog, .]|\n",
            "+---------------------------------------+--------------------------------------+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When caseSensitive is False (default value) , the spellchecker innores the uppercase included in the external dictionary. When it is set to True, only the uppercased version of the word is corrected."
      ],
      "metadata": {
        "id": "jFnKPEIk04Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setDupsLimit"
      ],
      "metadata": {
        "id": "Ymz9Wuq11_bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "spellchecker_1 = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell_1\") \\\n",
        ".setDictionary(\"external_dict.txt\") \\\n",
        ".setDupsLimit(1)\n",
        "\n",
        "spellchecker_2 = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell_2\") \\\n",
        ".setDictionary(\"external_dict.txt\") \\\n",
        ".setDupsLimit(0)\n",
        "\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "documentAssembler,\n",
        "tokenizer,\n",
        "spellchecker_1,\n",
        "spellchecker_2\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "spellcheck_model = pipeline.fit(empty_df)\n",
        "\n",
        "text_df = spark.createDataFrame([[\"It was a gooood dogh.\"]]).toDF(\"text\")\n",
        "\n",
        "corrected_text = spellcheck_model.transform(text_df)\n",
        "\n",
        "corrected_text.select(col(\"token.result\").alias(\"before_spellchecker\"), col(\"spell_1.result\").alias(\"dups_limit_1\"), col(\"spell_2.result\").alias(\"dups_limit_0\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRNl9Zod0axC",
        "outputId": "07977640-94ff-45fb-c93f-1afed1d4a00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+--------------------------+-------------------------+\n",
            "|before_spellchecker          |dups_limit_1              |dups_limit_0             |\n",
            "+-----------------------------+--------------------------+-------------------------+\n",
            "|[It, was, a, gooood, dogh, .]|[It, was, a, good, dog, .]|[It, was, a, god, dog, .]|\n",
            "+-----------------------------+--------------------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##setWordSizeIgnore"
      ],
      "metadata": {
        "id": "B6d9uKk9y-pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "spellchecker_1 = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell_1\") \\\n",
        ".setDictionary(\"external_dict.txt\") \\\n",
        ".setWordSizeIgnore(3)\n",
        "\n",
        "spellchecker_2 = NorvigSweetingApproach() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"spell_2\") \\\n",
        ".setDictionary(\"external_dict.txt\") \\\n",
        ".setWordSizeIgnore(4)\n",
        "\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "documentAssembler,\n",
        "tokenizer,\n",
        "spellchecker_1,\n",
        "spellchecker_2\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "spellcheck_model = pipeline.fit(empty_df)\n",
        "\n",
        "text_df = spark.createDataFrame([[\"It was a good dogh.\"]]).toDF(\"text\")\n",
        "\n",
        "corrected_text = spellcheck_model.transform(text_df)\n",
        "\n",
        "corrected_text.select(col(\"token.result\").alias(\"before_spellchecker\"), col(\"spell_1.result\").alias(\"word_size_ignore_3\"), col(\"spell_2.result\").alias(\"word_size_ignore_4\")).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "y9zZxFI-3ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fec1847-be49-42d4-b899-7dfeb565f7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------+--------------------------+---------------------------+\n",
            "|before_spellchecker        |word_size_ignore_3        |word_size_ignore_4         |\n",
            "+---------------------------+--------------------------+---------------------------+\n",
            "|[It, was, a, good, dogh, .]|[It, was, a, good, dog, .]|[It, was, a, good, dogh, .]|\n",
            "+---------------------------+--------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the misspelled word has 4 characters. The spellchecker with a value for wordSizeIgnore of 4 ignored this token and did not correct its spelling."
      ],
      "metadata": {
        "id": "I61hihZ7Nfi5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}