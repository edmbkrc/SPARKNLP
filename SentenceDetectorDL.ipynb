{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YynYxK8Jc1tM",
        "outputId": "2f30e043-01b8-4915-bad6-5ea0f23b1525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.5/579.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "from pyspark.ml import PipelineModel\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "spark = sparknlp.start()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "1648-ye7t_6-",
        "outputId": "3923901c-fb0f-4b8d-ea84-b03ce4ec5b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cd0ee9ca0e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4aa928fce875:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\")\n",
        "\n",
        "sd_pipeline = PipelineModel(stages=[documenter, sentencerDL])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMIeiD7CuzFo",
        "outputId": "b2380ccd-5319-4c89-d4da-0a599014aa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_model = LightPipeline(sd_pipeline)"
      ],
      "metadata": {
        "id": "QJJ1JNiGvlRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"John loves Mary.mary loves Peter\n",
        "          Peter loves Helen .Helen loves John;\n",
        "          Total: four. people involved.\"\"\"\n",
        "\n",
        "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "  print(anno.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uIol7AlvuIM",
        "outputId": "9fca1e15-acd1-4e5e-b784-6203f07acbd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John loves Mary.\n",
            "mary loves Peter\n",
            "Peter loves Helen .\n",
            "Helen loves John;\n",
            "Total: four. people involved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"John loves Mary.mary loves Peter\n",
        "          Peter loves Helen .Helen loves John;\n",
        "          Total: four. people involved.\"\"\"\n",
        "\n",
        "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwW2oBDgwD6R",
        "outputId": "ec0288d4-e1e6-45d2-b08f-5565f26dc2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0\t15\tJohn loves Mary.\n",
            "1\t16\t31\tmary loves Peter\n",
            "2\t43\t61\tPeter loves Helen .\n",
            "3\t62\t78\tHelen loves John;\n",
            "4\t91\t119\tTotal: four. people involved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Testing with a Broken Text (random \\n chars added)"
      ],
      "metadata": {
        "id": "o4Snp-qDwWH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "There are many NLP tasks like text summarization, question-answering, sentence prediction to name a few. One method to get\\n these tasks done is using a pre-trained model. Instead of training\n",
        "a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is\n",
        "then fine-tuned for each NLP tasks according to need.\n",
        "Let’s just peek into the pre-BERT world…\n",
        "For creating models, we need words to be represented in a form \\n understood by the training network, ie, numbers. Thus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
        "One of the earliest algorithms used for this purpose is word2vec. However, the drawback of word2vec models was that they were context-free. One problem caused by this is that they cannot accommodate polysemy. For example, the word ‘letter’ has a different meaning according to the context. It can mean ‘single element of alphabet’ or ‘document addressed to another person’. But in word2vec both the letter returns same embeddings.\n",
        "'''\n",
        "\n",
        "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n',''))) # removing \\n to beutify printing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqXfS4Y3wSkF",
        "outputId": "d91ad39b-1238-426e-cf79-990f49a8febc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t1\t104\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few.\n",
            "1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n",
            "2\t172\t362\tInstead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data.\n",
            "3\t364\t398\tThis is called a pre-trained model.\n",
            "4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n",
            "5\t481\t520\tLet’s just peek into the pre-BERT world…\n",
            "6\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n",
            "7\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
            "8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n",
            "9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n",
            "10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n",
            "11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n",
            "12\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n",
            "13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentencerDL.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0uf7q7CwgVk",
        "outputId": "53834889-0f70-4b35-8d4f-6a22cc494e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='customBounds', doc='characters used to explicitly mark sentence bounds'): [],\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='engine', doc='Deep Learning engine used for this model'): 'tensorflow',\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='explodeSentences', doc='whether to explode each sentence into a different row, for better parallelization. Defaults to false.'): False,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='maxLength', doc='Set the maximum allowed length for each sentence'): 99999,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='minLength', doc='Set the minimum allowed length for each sentence.'): 0,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='splitLength', doc='length at which sentences will be forcibly split.'): 2147483647,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='storageRef', doc='storage unique identifier'): 'SentenceDetectorDLModel_c83c27f46b97',\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='useCustomBoundsOnly', doc='Only utilize custom bounds in sentence detection'): False,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='Encoder', doc='Data encoder'): JavaObject id=o60,\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='impossiblePenultimates', doc=\"Impossible penultimates - list of strings which a sentence can't end with\"): ['Bros',\n",
              "  'No',\n",
              "  'al',\n",
              "  'vs',\n",
              "  'etc',\n",
              "  'Fig',\n",
              "  'Dr',\n",
              "  'Prof',\n",
              "  'PhD',\n",
              "  'MD',\n",
              "  'Co',\n",
              "  'Corp',\n",
              "  'Inc',\n",
              "  'bros',\n",
              "  'VS',\n",
              "  'Vs',\n",
              "  'ETC',\n",
              "  'fig',\n",
              "  'dr',\n",
              "  'prof',\n",
              "  'PHD',\n",
              "  'phd',\n",
              "  'md',\n",
              "  'co',\n",
              "  'corp',\n",
              "  'inc',\n",
              "  'Jan',\n",
              "  'Feb',\n",
              "  'Mar',\n",
              "  'Apr',\n",
              "  'Jul',\n",
              "  'Aug',\n",
              "  'Sep',\n",
              "  'Sept',\n",
              "  'Oct',\n",
              "  'Nov',\n",
              "  'Dec',\n",
              "  'St',\n",
              "  'st',\n",
              "  'AM',\n",
              "  'PM',\n",
              "  'am',\n",
              "  'pm',\n",
              "  'e.g',\n",
              "  'f.e',\n",
              "  'i.e'],\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='modelArchitecture', doc='Model architecture (CNN)'): 'cnn',\n",
              " Param(parent='SentenceDetectorDLModel_c83c27f46b97', name='outputCol', doc='output annotation column. can be left default.'): 'sentences'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setMaxLength"
      ],
      "metadata": {
        "id": "yonLjbfixMP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = '''\n",
        "There are many NLP tasks like text summarization, question-answering, sentence prediction to name a few. One method to get\\n these tasks done is using a pre-trained model. Instead of training\n",
        "a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is\n",
        "then fine-tuned for each NLP tasks according to need.\n",
        "Let’s just peek into the pre-BERT world…\n",
        "For creating models, we need words to be represented in a form \\n understood by the training network, ie, numbers. Thus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
        "One of the earliest algorithms used for this purpose is word2vec. However, the drawback of word2vec models was that they were context-free. One problem caused by this is that they cannot accommodate polysemy. For example, the word ‘letter’ has a different meaning according to the context. It can mean ‘single element of alphabet’ or ‘document addressed to another person’. But in word2vec both the letter returns same embeddings.\n",
        "'''"
      ],
      "metadata": {
        "id": "r4d3-UwexAaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\")\\\n",
        "  .setMaxLength(80)\n",
        "\n",
        "sd_pipeline2 = PipelineModel(stages=[documenter, sentencerDL])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yJ4rFi9xQjK",
        "outputId": "d05dd4f5-47bc-42d4-ae22-d4eedfe4ded9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_model2 = LightPipeline(sd_pipeline2)"
      ],
      "metadata": {
        "id": "hICqPFQixlDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for anno in sd_model2.fullAnnotate(text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n',''))) # removing \\n to beutify printing|\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUo-71sUxpKW",
        "outputId": "dc783318-bc79-462a-f1c2-f0784dd26f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n",
            "3\t364\t398\tThis is called a pre-trained model.\n",
            "4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n",
            "5\t481\t520\tLet’s just peek into the pre-BERT world…\n",
            "8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n",
            "9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n",
            "10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n",
            "11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n",
            "13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using the setMaxLength parameter, number of characters a sentence can have is limited, as shown by the less number of detected sentences."
      ],
      "metadata": {
        "id": "EOwMdC8_yKCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setMinLength"
      ],
      "metadata": {
        "id": "iF2k-Kn5yL--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\")\\\n",
        "  .setMaxLength(1000)\\\n",
        "  .setMinLength(50)\n",
        "\n",
        "sd_pipeline3 = PipelineModel(stages=[documenter, sentencerDL])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqP8eOIuxsv4",
        "outputId": "a9d86d0b-0e6b-4f5f-9b2a-b752a9d2eb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_model3 = LightPipeline(sd_pipeline3)"
      ],
      "metadata": {
        "id": "UCCSFzldya4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for anno in sd_model3.fullAnnotate(text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n','')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAG3E5T6yfZv",
        "outputId": "9c7ba936-d29f-455c-d3ff-56995e8a5c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t1\t104\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few.\n",
            "1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n",
            "2\t172\t362\tInstead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data.\n",
            "4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n",
            "6\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n",
            "7\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
            "8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n",
            "9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n",
            "10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n",
            "11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n",
            "12\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n",
            "13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using the setMinLength parameter, number of characters a sentence can have is limited."
      ],
      "metadata": {
        "id": "x0wyT53VyqEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setCustomBounds and setUseCustomBoundsOnly"
      ],
      "metadata": {
        "id": "eqf5fU6wysVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\")\\\n",
        "  .setMaxLength(1000)\\\n",
        "  .setMinLength(0)\\\n",
        "  .setCustomBounds([\"!!\"])\\\n",
        "  .setUseCustomBoundsOnly(True)\n",
        "\n",
        "sd_pipeline4 = PipelineModel(stages=[documenter, sentencerDL])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eQdB38yyhqZ",
        "outputId": "98db5ec7-52b1-40ae-b166-9a855c357a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = '''\n",
        "There are many NLP tasks like text summarization, question-answering, sentence prediction to name a few!! One method to get these tasks done is using a pre-trained model. Instead of training\n",
        "a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is\n",
        "then fine-tuned for each NLP tasks according to need.\n",
        "Let’s just peek into the pre-BERT world…\n",
        "For creating models, we need words to be represented in a form understood by the training network, ie, numbers. Thus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
        "One of the earliest algorithms used for this purpose is word2vec. However, the drawback of word2vec models was that they were context-free. One problem caused by this is that they cannot accommodate polysemy. For example, the word ‘letter’ has a different meaning according to the context. It can mean ‘single element of alphabet’ or ‘document addressed to another person’. But in word2vec both the letter returns same embeddings.\n",
        "'''"
      ],
      "metadata": {
        "id": "YXh9PT21zoZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd_model4 = LightPipeline(sd_pipeline4)"
      ],
      "metadata": {
        "id": "BThmhbwVzQ0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for anno in sd_model4.fullAnnotate(text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n','')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ahC9D6zzY6N",
        "outputId": "fa4550ec-3902-4e03-d77a-3b58b5c1b81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t1\t105\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, there is one sentence ending with '!!' and the annotator is expected to detect only this sentence."
      ],
      "metadata": {
        "id": "1eDqrX-Yz1m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setSplitLength"
      ],
      "metadata": {
        "id": "69hfG2Cwz5qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "setSplitLength parameter can be used to set the length at which sentences will be forcibly split. It is ignored if not set."
      ],
      "metadata": {
        "id": "ehQhETChz-6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "There are many NLP tasks like text summarization, question-answering, sentence prediction to name a few. One method to get\\n these tasks done is using a pre-trained model. Instead of training\n",
        "a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is\n",
        "then fine-tuned for each NLP tasks according to need.\n",
        "Let’s just peek into the pre-BERT world…\n",
        "For creating models, we need words to be represented in a form \\n understood by the training network, ie, numbers. Thus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
        "One of the earliest algorithms used for this purpose is word2vec. However, the drawback of word2vec models was that they were context-free. One problem caused by this is that they cannot accommodate polysemy. For example, the word ‘letter’ has a different meaning according to the context. It can mean ‘single element of alphabet’ or ‘document addressed to another person’. But in word2vec both the letter returns same embeddings.\n",
        "'''"
      ],
      "metadata": {
        "id": "4jx3lfCAza79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\")\\\n",
        "  .setMaxLength(1000)\\\n",
        "  .setMinLength(0)\\\n",
        "  .setSplitLength(50) \\\n",
        "  .setCustomBounds([])\\\n",
        "  .setUseCustomBoundsOnly(False)\n",
        "\n",
        "\n",
        "sd_pipeline5 = PipelineModel(stages=[documenter, sentencerDL])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aie-_lSU0CzR",
        "outputId": "35c2055c-54dc-4fdb-81dc-6746937e3709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_model5 = LightPipeline(sd_pipeline5)"
      ],
      "metadata": {
        "id": "n9ObmSYk0fwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for anno in sd_model5.fullAnnotate(text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n','')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xKE2KyS0iuY",
        "outputId": "ae981c7d-b252-47c5-ad8f-bdc08f85d1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t1\t49\tThere are many NLP tasks like text summarization,\n",
            "1\t50\t98\tquestion-answering, sentence prediction to name a\n",
            "2\t99\t102\tfew.\n",
            "3\t106\t151\tOne method to get these tasks done is using a\n",
            "4\t152\t169\tpre-trained model.\n",
            "5\t172\t220\tInstead of training a model from scratch for NLP\n",
            "6\t221\t270\ttasks using millions of annotated texts each time,\n",
            "7\t271\t317\ta general language representation is created by\n",
            "8\t318\t359\ttraining a model on a huge amount of data.\n",
            "9\t364\t398\tThis is called a pre-trained model.\n",
            "10\t400\t445\tThis pre-trained model is then fine-tuned for\n",
            "11\t446\t478\teach NLP tasks according to need.\n",
            "12\t481\t520\tLet’s just peek into the pre-BERT world…\n",
            "13\t522\t561\tFor creating models, we need words to be\n",
            "14\t562\t611\trepresented in a form  understood by the training\n",
            "15\t612\t632\tnetwork, ie, numbers.\n",
            "16\t636\t682\tThus many algorithms were used to convert words\n",
            "17\t683\t730\tinto vectors or more precisely, word embeddings.\n",
            "18\t734\t777\tOne of the earliest algorithms used for this\n",
            "19\t778\t797\tpurpose is word2vec.\n",
            "20\t800\t848\tHowever, the drawback of word2vec models was that\n",
            "21\t849\t871\tthey were context-free.\n",
            "22\t874\t919\tOne problem caused by this is that they cannot\n",
            "23\t920\t940\taccommodate polysemy.\n",
            "24\t943\t988\tFor example, the word ‘letter’ has a different\n",
            "25\t989\t1021\tmeaning according to the context.\n",
            "26\t1024\t1066\tIt can mean ‘single element of alphabet’ or\n",
            "27\t1067\t1105\t‘document addressed to another person’.\n",
            "28\t1108\t1151\tBut in word2vec both the letter returns same\n",
            "29\t1152\t1162\tembeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effect of using the setSplitLength to limiting the length of a sentence to 50 characters can be seen above."
      ],
      "metadata": {
        "id": "D39IHm7z02Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setImpossiblePenultimates"
      ],
      "metadata": {
        "id": "4JXVv_Rx054-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "setImpossiblePenultimates parameter can be used to define a list of strings which a sentence can’t end with. It is ignored and Default List is used if not set."
      ],
      "metadata": {
        "id": "gxNG_tud1AR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\") \\\n",
        "  .setMaxLength(1000)\\\n",
        "  .setMinLength(0)\\\n",
        "  .setSplitLength(1000) \\\n",
        "  .setCustomBounds([])\\\n",
        "  .setUseCustomBoundsOnly(False)\\\n",
        "  .setImpossiblePenultimates([\"few\", \"data\", \"model\"])\n",
        "\n",
        "sd_pipeline6 = PipelineModel(stages=[documenter, sentencerDL])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxcKEK9W0kxs",
        "outputId": "1ff31a5a-734a-4953-a784-088d759833ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sd_model6 = LightPipeline(sd_pipeline6)"
      ],
      "metadata": {
        "id": "7ukQYai41cBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for anno in sd_model6.fullAnnotate(text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n','')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30NWX8EH1fBQ",
        "outputId": "af495b0f-cdfd-4676-e1de-cc2c8a1a48e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t1\t479\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few. One method to get these tasks done is using a pre-trained model. Instead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is then fine-tuned for each NLP tasks according to need.\n",
            "1\t481\t520\tLet’s just peek into the pre-BERT world…\n",
            "2\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n",
            "3\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
            "4\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n",
            "5\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n",
            "6\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n",
            "7\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n",
            "8\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n",
            "9\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined a short list of strings which tells the model to never end a sentence when those words/strings are encountered.\n",
        "\n",
        "Although there is a full stop after them, those words were ignored during sentence boundary detection."
      ],
      "metadata": {
        "id": "8cHpShFJ11dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilanguage SentenceDetectorDL"
      ],
      "metadata": {
        "id": "Yxd1jCIc13_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentencerDL_multilang = SentenceDetectorDLModel\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"xx\") \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"sentences\")\n",
        "\n",
        "sd_pipeline_multi = PipelineModel(stages=[documenter, sentencerDL_multilang])\n",
        "\n",
        "sd_model_multi = LightPipeline(sd_pipeline_multi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gebcfh91hhL",
        "outputId": "ab49b9e9-6ef5-4ca9-ccf2-e03431628aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_text = \"\"\"\n",
        "Metin özetleme, soru-cevaplama, cümle tahmini gibi birçok NLP görevi vardır.\n",
        "Bu görevleri gerçekleştirmek için kullanılan bir yöntem, önceden eğitilmiş bir model kullanmaktır.\n",
        "Her seferinde milyonlarca etiketlenmiş metinle sıfırdan bir model eğitmek yerine, büyük miktarda veri üzerinde bir model eğitilerek genel bir dil temsili oluşturulur.\n",
        "Buna önceden eğitilmiş model denir. Bu önceden eğitilmiş model, her NLP görevi için ihtiyaçlara göre ince ayar yapılır. BERT öncesi dünyaya bir göz atalım...\n",
        "Modeller oluşturmak için, kelimelerin eğitim ağı tarafından anlaşılan bir formda, yani sayılarla temsil edilmesi gerekir.\n",
        "Bu nedenle, kelimeleri vektörlere veya daha doğrusu kelime yerleştirmelerine dönüştürmek için birçok algoritma kullanılmıştır.\n",
        "Bu amaçla kullanılan en erken algoritmalardan biri word2vec'tir. Ancak word2vec modellerinin dezavantajı, bağlamdan bağımsız olmalarıydı.\n",
        "Bu durumun neden olduğu bir sorun, çok anlamlılığı barındıramamalarıdır. Örneğin, 'mektup' kelimesi bağlama göre farklı anlamlar taşır.\n",
        "'Alfabenin tek bir öğesi' veya 'başka bir kişiye hitaben yazılmış belge' anlamına gelebilir.\n",
        "Ancak word2vec'te her iki durumda da mektup aynı yerleştirmeleri döndürür.\n",
        "\"\"\"\n",
        "\n",
        "for anno in sd_model_multi.fullAnnotate(tr_text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOO71fQL2WUs",
        "outputId": "2d3bd6bb-788f-45eb-8b01-35886e58142e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tMetin özetleme, soru-cevaplama, cümle tahmini gibi birçok NLP görevi vardır.\n",
            "1\tBu görevleri gerçekleştirmek için kullanılan bir yöntem, önceden eğitilmiş bir model kullanmaktır.\n",
            "2\tHer seferinde milyonlarca etiketlenmiş metinle sıfırdan bir model eğitmek yerine, büyük miktarda veri üzerinde bir model eğitilerek genel bir dil temsili oluşturulur.\n",
            "3\tBuna önceden eğitilmiş model denir.\n",
            "4\tBu önceden eğitilmiş model, her NLP görevi için ihtiyaçlara göre ince ayar yapılır.\n",
            "5\tBERT öncesi dünyaya bir göz atalım.\n",
            "6\t..\n",
            "7\tModeller oluşturmak için, kelimelerin eğitim ağı tarafından anlaşılan bir formda, yani sayılarla temsil edilmesi gerekir.\n",
            "8\tBu nedenle, kelimeleri vektörlere veya daha doğrusu kelime yerleştirmelerine dönüştürmek için birçok algoritma kullanılmıştır.\n",
            "9\tBu amaçla kullanılan en erken algoritmalardan biri word2vec'tir.\n",
            "10\tAncak word2vec modellerinin dezavantajı, bağlamdan bağımsız olmalarıydı.\n",
            "11\tBu durumun neden olduğu bir sorun, çok anlamlılığı barındıramamalarıdır.\n",
            "12\tÖrneğin, 'mektup' kelimesi bağlama göre farklı anlamlar taşır.\n",
            "13\t'Alfabenin tek bir öğesi' veya 'başka bir kişiye hitaben yazılmış belge' anlamına gelebilir.\n",
            "14\tAncak word2vec'te her iki durumda da mektup aynı yerleştirmeleri döndürür.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gr_text= '''\n",
        "Όπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει\n",
        "λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται. Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη\n",
        "λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n",
        "Προφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι\n",
        "οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη. Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η\n",
        "εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές. Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα\n",
        "ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n",
        "'''\n",
        "\n",
        "for anno in sd_model_multi.fullAnnotate(gr_text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CpSKGeg28rW",
        "outputId": "b0eb377f-f3a6-4237-a414-2d7882427d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tΌπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n",
            "1\tΣτη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n",
            "2\tΠροφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n",
            "3\tΌσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n",
            "4\tΤα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cyrillic_text = '''\n",
        "B чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e\n",
        "въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n",
        "Πoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe,\n",
        "ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n",
        "Taнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n",
        "Πoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd,\n",
        "ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n",
        "Al aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n",
        "Cpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa\n",
        "c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa\n",
        "дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n",
        "'''\n",
        "\n",
        "for anno in sd_model_multi.fullAnnotate(cyrillic_text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crni8xhb3Ivl",
        "outputId": "be922a9b-24e3-435c-935f-c2f5d082bf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tB чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n",
            "1\tΠoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n",
            "2\tTaнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n",
            "3\tΠoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n",
            "4\tAl aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n",
            "5\tCpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spanish_text= '''\n",
        "Actualmente, la Hispanidad se celebra dentro y fuera de España,\n",
        "aunque es una de las fiestas que más polémica generan.\n",
        "En muchos países de Latinoamérica el descubrimiento de América\n",
        "se asocia al comienzo de la colonización española y a la destrucción de las culturas locales nativas.\n",
        "Por este motivo, en América del Sur la fiesta\n",
        "se percibe como una reivindicación.\n",
        "En España la Hispanidad se festeja\n",
        "con un desfile militar y una recepción, encabezada por los Reyes,\n",
        "para el cuerpo diplomático en el Palacio Real.\n",
        "'''\n",
        "\n",
        "for anno in sd_model_multi.fullAnnotate(spanish_text)[0][\"sentences\"]:\n",
        "\n",
        "    print(\"{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96bpo7tJ3MIh",
        "outputId": "4f59ca9f-d703-4d05-cc3f-6b43375f4acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tActualmente, la Hispanidad se celebra dentro y fuera de España, aunque es una de las fiestas que más polémica generan.\n",
            "1\tEn muchos países de Latinoamérica el descubrimiento de América se asocia al comienzo de la colonización española y a la destrucción de las culturas locales nativas.\n",
            "2\tPor este motivo, en América del Sur la fiesta se percibe como una reivindicación.\n",
            "3\t En España la Hispanidad se festeja con un desfile militar y una recepción, encabezada por los Reyes, para el cuerpo diplomático en el Palacio Real.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eNl6xAe3Ofe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}