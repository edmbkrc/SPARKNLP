{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP0KG8PFdUCE",
        "outputId": "d449a7aa-6110-4bba-a625-575f3767f647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spark-nlp\n",
            "  Downloading spark_nlp-5.5.0-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading spark_nlp-5.5.0-py2.py3-none-any.whl (620 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=678c271bab49f8519386bfd2be8d0b2afdaaca58e7123d5380dcd85b66ec0daf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: spark-nlp, pyspark\n",
            "Successfully installed pyspark-3.5.3 spark-nlp-5.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "cwE27PgPdhQO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .addException()"
      ],
      "metadata": {
        "id": "RzWAtVSyeXJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        ".setInputCols([\"document\"])\\\n",
        ".setOutputCol(\"token\")\\\n",
        ".addException(\"Samsun\")\\\n",
        ".addException(\"New York\")\\\n",
        ".setCaseSensitiveExceptions(False)\n",
        "\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, tokenizer])\n",
        "\n",
        "text = \"Samsun! is a city in Turkey.\"\n",
        "text1 = \"Peter Parker (Spiderman?) is a nice guy and lives in New York or new york but has no e-mail!\"\n",
        "\n",
        "df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "df1 = spark.createDataFrame([[text1]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(df).transform(df)\n",
        "result1 = pipeline.fit(df1).transform(df1)\n",
        "\n",
        "result.select(\"token.result\").show(truncate=False)\n",
        "result1.select(\"token.result\").take(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ykMoktGdwY5",
        "outputId": "019d9dca-1fb7-4d00-eb88-6386a8a7b97d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------+\n",
            "|result                                 |\n",
            "+---------------------------------------+\n",
            "|[Samsun, !, is, a, city, in, Turkey, .]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(', 'Spiderman', '?)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'or', 'new york', 'but', 'has', 'no', 'e-mail', '!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\") \\\n",
        "    .setSplitChars(['-']) \\\n",
        "    .setContextChars(['?', '!'])\\\n",
        "    .addException(\"New York\")\\\n",
        "    .setCaseSensitiveExceptions(True)\\\n",
        "    .setSuffixPattern(\"([a])\\z\")\\\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, tokenizer])\n",
        "\n",
        "text = \"Samsun is a citya in Turkey. Another one is New York.\"\n",
        "text1 = \"Peter Parker (Spiderman?) is a nice guy and liv-es in New York or new york but has no e-mail!\"\n",
        "\n",
        "df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "df1 = spark.createDataFrame([[text1]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(df).transform(df)\n",
        "result1 = pipeline.fit(df1).transform(df1)\n",
        "\n",
        "result.select(\"token.result\").show(truncate=False)\n",
        "result1.select(\"token.result\").take(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbVjV0OgjWO9",
        "outputId": "d7ec1749-108f-44e1-bbf8-b955a54a2a76"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------+\n",
            "|result                                                            |\n",
            "+------------------------------------------------------------------+\n",
            "|[Samsun, is, a, city, a, in, Turkey., Another, one, is, New York.]|\n",
            "+------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(Spiderman?)', 'is', 'a', 'nice', 'guy', 'and', 'liv-es', 'in', 'New York', 'or', 'new', 'york', 'but', 'has', 'no', 'e-mail!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\") \\\n",
        "    .setPrefixPattern(\"\\A([a])\")\\\n",
        "    .setSplitChars(['-']) \\\n",
        "    .setContextChars(['?', '!'])\\\n",
        "    .addException(\"New York\")\\\n",
        "    .setCaseSensitiveExceptions(True)\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, tokenizer])\n",
        "\n",
        "text = \"Samsun is a citya in Turkey. another one is New York.Another one is Paris.\"\n",
        "text1 = \"Peter Parker (Spiderman?) is a nice guy and liv-es in New York or new york but has no e-mail!\"\n",
        "\n",
        "df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "df1 = spark.createDataFrame([[text1]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(df).transform(df)\n",
        "result1 = pipeline.fit(df1).transform(df1)\n",
        "\n",
        "result.select(\"token.result\").show(truncate=False)\n",
        "result1.select(\"token.result\").take(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k45lKs2jUyW",
        "outputId": "d0c71f6e-513c-4bfa-b3d7-65ea00533891"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|result                                                                                    |\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[Samsun, is, a, citya, in, Turkey., a, nother, one, is, New York.Another, one, is, Paris.]|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(Spiderman?)', 'is', 'a', 'nice', 'guy', 'a', 'nd', 'liv-es', 'in', 'New York', 'or', 'new', 'york', 'but', 'has', 'no', 'e-mail!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\") \\\n",
        "    .setSplitChars(['-']) \\\n",
        "    .setContextChars(['?', '!'])\\\n",
        "    .addException(\"New York\")\\\n",
        "    .setCaseSensitiveExceptions(True)\\\n",
        "    .setMinLength(2)\\\n",
        "    .setMaxLength(5)\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, tokenizer])\n",
        "\n",
        "text = \"Samsun is a citya in Turkey. another one is New York.Another one is Paris.\"\n",
        "text1 = \"Peter Parker (Spiderman?) is a nice guy and liv-es in New York or new york but has no e-mail!\"\n",
        "\n",
        "df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "df1 = spark.createDataFrame([[text1]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(df).transform(df)\n",
        "result1 = pipeline.fit(df1).transform(df1)\n",
        "\n",
        "result.select(\"token.result\").show(truncate=False)\n",
        "result1.select(\"token.result\").take(1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN1j4n71fFiH",
        "outputId": "d67ce661-c381-4511-c1f4-c55dac804403"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+\n",
            "|result                           |\n",
            "+---------------------------------+\n",
            "|[is, citya, in, one, is, one, is]|\n",
            "+---------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'is', 'nice', 'guy', 'and', 'liv', 'es', 'in', 'or', 'new', 'york', 'but', 'has', 'no', 'mail'])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZ7pFgg9m0z1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}